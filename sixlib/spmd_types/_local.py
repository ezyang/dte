"""Local SPMD type coercion operations: reinterpret and convert."""

from __future__ import annotations

from typing import TYPE_CHECKING

import torch
from sixlib.spmd_types import _dist
from sixlib.spmd_types._mesh import _get_mesh_axis_group
from sixlib.spmd_types.types import (
    _canonicalize_shard,
    I,
    P,
    PerMeshAxisSpmdType,
    R,
    Shard,
    V,
)
from torch.distributed._local_tensor import local_tensor_mode, LocalTensor
from torch.overrides import handle_torch_function, has_torch_function_unary

if TYPE_CHECKING:
    from torch.distributed import ProcessGroup

# =============================================================================
# reinterpret autograd Functions
# =============================================================================


class _ReplicateToVarying(torch.autograd.Function):
    """reinterpret(R,V): R -> V, backward is reinterpret(V,P): V -> P (no-op)."""

    @staticmethod
    def forward(ctx, x, axis):
        ctx.axis = axis
        return x

    @staticmethod
    def backward(ctx, grad_out):
        # reinterpret(V,P) is a no-op in forward direction
        return grad_out, None


class _ReplicateToInvariant(torch.autograd.Function):
    """reinterpret(R,I): R -> I, backward is convert(I,P): I -> P."""

    @staticmethod
    def forward(ctx, x, axis):
        ctx.axis = axis
        return x  # no-op in forward

    @staticmethod
    def backward(ctx, grad_out):
        # backward is convert(I,P): I -> P
        # Zero out all but rank 0
        pg = _get_mesh_axis_group(ctx.axis)

        mode = local_tensor_mode()
        if mode is not None and isinstance(grad_out, LocalTensor):
            return mode.tensor_map(
                grad_out, lambda r, t: _replicate_to_partial_fwd(t, r)
            ), None
        else:
            rank = _dist.dist.get_rank(pg)
            return _replicate_to_partial_fwd(grad_out, rank), None


class _InvariantToReplicate(torch.autograd.Function):
    """reinterpret(I,R): I -> R, backward is all_reduce(I): P -> I."""

    @staticmethod
    def forward(ctx, x, axis):
        ctx.axis = axis
        return x  # no-op in forward

    @staticmethod
    def backward(ctx, grad_out):
        # backward is all_reduce(I): P -> I
        from sixlib.spmd_types._collectives import all_reduce  # @manual

        return all_reduce(grad_out, ctx.axis, src=P, dst=I), None


class _VaryingToPartial(torch.autograd.Function):
    """reinterpret(V,P): V -> P, backward is reinterpret(R,V): R -> V (no-op)."""

    @staticmethod
    def forward(ctx, x, axis):
        ctx.axis = axis
        return x  # no-op in forward

    @staticmethod
    def backward(ctx, grad_out):
        # backward is reinterpret(R,V): R -> V (no-op)
        return grad_out, None


class _ReplicateToPartial(torch.autograd.Function):
    """reinterpret(R,P): R -> P, backward is reinterpret(R,P): R -> P."""

    @staticmethod
    def forward(ctx, x, axis):
        ctx.axis = axis
        return x  # no-op in forward

    @staticmethod
    def backward(ctx, grad_out):
        # backward is reinterpret(R,P): same as forward (no-op)
        return grad_out, None


def reinterpret(  # noqa: C901
    x,
    axis: str | ProcessGroup,
    *,
    src: PerMeshAxisSpmdType,
    dst: PerMeshAxisSpmdType,
    expert_mode: bool = False,
):
    """``reinterpret(x, mesh_axis, src, dst)``

    Coerce from one local SPMD type to another local SPMD type without changing
    the local tensor.  It is guaranteed to be a no-op in forwards.

    **Important:** Unlike ``convert``, ``reinterpret`` can change the semantic
    value of a tensor.  For example, ``reinterpret(R,P)`` treats a replicated
    value as if it were partial, meaning after reduction you get N times the
    original value (where N is the mesh axis size).  If you want to preserve
    semantics, use ``convert``.

    This API does not support shard for src/dst, because the restriction on no
    local tensor change means that the local SPMD semantics would be precisely
    the same as the corresponding varying operation.

    Args:
        x: Input tensor
        axis: The mesh axis to operate on (string name or ProcessGroup)
        src: Source local SPMD type (R, I, V, P)
        dst: Destination local SPMD type (R, I, V, P)

    **Supported coercions:**

    ``reinterpret(R,I): R -> I``, the backwards is ``convert(I,P): I -> P``::

        def reinterpret_R_I_spec(x: f32[*shape]) -> f32[*shape]:
            return x

        Forward:
        A  =>  A

        Backward:
        +A
        +0  <=  A
        +0

    ``reinterpret(R,V): R -> V``, the backwards is ``reinterpret(V,P): V -> P``::

        def reinterpret_R_V_spec(x: f32[*shape]) -> List[f32[*shape]]:
            # Makes N copies of the input
            return [x] * mesh_axis_size

        Forward:
               A
        A  =>  A
               A

        Backward:
        +A      A
        +B  <=  B
        +C      C

    ``reinterpret(I,R): I -> R``, the backwards is ``all_reduce(I): P -> I``::

        def reinterpret_I_R_spec(x: f32[*shape]) -> f32[*shape]:
            return x

        Forward:
        A => A

        Backward:
                          +Ax
        Ax + Ay + Az  <=  +Ay
                          +Az

    ``reinterpret(V,P): V -> P``, the backwards is ``reinterpret(R,V): R -> V``::

        def reinterpret_V_P_spec(xs: List[f32[*shape]]) -> f32[*shape]:
            # Semantically does a sum, even if physically it hasn't happened yet!
            return sum(xs)

        Forward:
        A      +A
        B  =>  +B
        C      +C

        Backward:
        A
        A  <=  A
        A

    ``reinterpret(R,P): R -> P``, the backwards is ``reinterpret(R,P): R -> P``::

        def reinterpret_R_P(x: f32[*shape]) -> f32[*shape]:
            # Summing each replicated entry together scales the value by axis size
            return x * mesh_axis_size

        Forward:
               +A
        A  =>  +A
               +A

        Backward:
        +A
        +A  <=  A
        +A

    ``reinterpret(I,V): I -> V`` is the composition of ``I -> R -> V``.
    ``reinterpret(R,P): R -> P`` is the composition of ``R -> V -> P``.
    ``reinterpret(I,P): I -> P`` is the composition of ``I -> R -> P``.  Note
    that these reinterprets have unusual semantics: the resulting tensor has
    been scaled by the mesh axis size (because you are now obligated to sum
    each of the (equal) quantities of the rank together!)  If you instead
    wanted to *preserve* the original semantic meaning of the tensor, use
    ``convert``.

    Here is a table of permissible reinterprets (``-`` is no-op, ``X`` is
    direct coercion, ``/`` is transitive coercion.)::

               dst
               R I V P
        src R  - X X /
            I  X - / /
            V      - X
            P        -
    """
    if has_torch_function_unary(x):
        return handle_torch_function(
            reinterpret,
            (x,),
            x,
            axis,
            src=src,
            dst=dst,
            expert_mode=expert_mode,
        )
    # Validate no Shard types
    if isinstance(src, Shard) or isinstance(dst, Shard):
        raise ValueError(
            f"reinterpret does not support S(i). Use V instead, or use convert for "
            f"semantics-preserving conversions. Got src={src}, dst={dst}"
        )

    if src is dst:
        return x  # no-op

    # Gate expert-only coercions
    if not expert_mode:
        if src is R and dst is I:
            raise ValueError(
                "reinterpret(R, I) requires expert_mode=True. "
                "This is rarely what you want in the forward pass; "
                "it exists as a backward for convert(I, P)."
            )
        if src is R and dst is P:
            raise ValueError(
                "reinterpret(R, P) requires expert_mode=True. "
                "This scales the semantic value by the mesh axis size, which is rarely "
                "intentional. If you want to preserve semantics, use convert(R, P) instead."
            )
        if src is R and dst is V:
            raise ValueError(
                "reinterpret(R, V) requires expert_mode=True. "
                "This makes N copies of the input, which is rarely what you want in the "
                "forward pass. If you want to shard a replicated value, use convert(R, V) "
                "or convert(R, S(i)) instead."
            )

    if src is R and dst is V:
        return _ReplicateToVarying.apply(x, axis)
    elif src is R and dst is I:
        return _ReplicateToInvariant.apply(x, axis)
    elif src is R and dst is P:
        return _ReplicateToPartial.apply(x, axis)
    elif src is I and dst is R:
        return _InvariantToReplicate.apply(x, axis)
    elif src is I and dst is V:
        # Composition: I -> R -> V
        return _ReplicateToVarying.apply(_InvariantToReplicate.apply(x, axis), axis)
    elif src is I and dst is P:
        # Composition: I -> R -> P
        return _ReplicateToPartial.apply(_InvariantToReplicate.apply(x, axis), axis)
    elif src is V and dst is P:
        return _VaryingToPartial.apply(x, axis)
    else:
        if src is P:
            raise ValueError(
                f"reinterpret({src}, {dst}) is not supported; it is semantically ill-defined. "
                "Call all_reduce(src=P, dst=R) first to materialize the sum, "
                "then do whatever conversion you need from R."
            )
        elif src is V:
            # V -> R or V -> I
            raise ValueError(
                f"reinterpret({src}, {dst}) is not supported. "
                f"We cannot unsafely assert that varying values on all ranks are actually the same. "
                f"Ensure your source is already {dst} instead."
            )
        else:
            raise ValueError(f"reinterpret({src}, {dst}) is not supported.")


# =============================================================================
# convert helper functions
# =============================================================================


def _replicate_to_varying_fwd(x, world_size, split_dim, rank, *, stack):
    """Forward: split and take local portion based on rank.

    Args:
        x: The replicated input tensor.
        world_size: Number of ranks in the process group.
        split_dim: The tensor dimension to split along.
        rank: The current rank's index.
        stack: If True, use select (unbind semantics) instead of chunk (shard semantics).
    """
    if stack:
        return x.select(split_dim, rank).contiguous()
    chunks = torch.chunk(x, world_size, dim=split_dim)
    return chunks[rank].contiguous()


def _varying_to_partial_fwd(x, world_size, split_dim, rank, *, stack):
    """Forward: pad with zeros, place data at rank position.

    Args:
        x: The varying input tensor (local shard).
        world_size: Number of ranks in the process group.
        split_dim: The tensor dimension to embed the shard into.
        rank: The current rank's index.
        stack: If True, use stack semantics (insert new dim) instead of concat semantics.
    """
    if stack:
        pad_shape = list(x.shape)
        pad_shape.insert(split_dim, world_size)
        result = torch.zeros(pad_shape, dtype=x.dtype, device=x.device)
        slices = [slice(None)] * len(pad_shape)
        slices[split_dim] = rank
        result[tuple(slices)] = x
        return result
    pad_shape = list(x.shape)
    pad_shape[split_dim] = pad_shape[split_dim] * world_size
    result = torch.zeros(pad_shape, dtype=x.dtype, device=x.device)
    chunk_size = x.shape[split_dim]
    slices = [slice(None)] * len(pad_shape)
    slices[split_dim] = slice(rank * chunk_size, (rank + 1) * chunk_size)
    result[tuple(slices)] = x
    return result


def _replicate_to_partial_fwd(x, rank):
    """Forward: keep value on rank 0, zero elsewhere.

    Args:
        x: The replicated input tensor.
        rank: The current rank's index. Only rank 0 keeps its value.
    """
    if rank == 0:
        return x.clone()
    else:
        return torch.zeros_like(x)


# =============================================================================
# convert autograd Functions
# =============================================================================


class _ConvertReplicateToVarying(torch.autograd.Function):
    """convert(R,V): R -> V, backward is convert(V,P): V -> P."""

    @staticmethod
    def forward(ctx, x, axis, split_dim, stack):
        ctx.axis = axis
        ctx.split_dim = split_dim
        ctx.stack = stack
        pg = _get_mesh_axis_group(axis)
        world_size = _dist.dist.get_world_size(pg)

        mode = local_tensor_mode()
        if mode is not None and isinstance(x, LocalTensor):
            return mode.tensor_map(
                x,
                lambda r, t: _replicate_to_varying_fwd(
                    t, world_size, split_dim, r, stack=stack
                ),
            )
        else:
            rank = _dist.dist.get_rank(pg)
            return _replicate_to_varying_fwd(
                x, world_size, split_dim, rank, stack=stack
            )

    @staticmethod
    def backward(ctx, grad_out):
        # backward is convert(V,P): V -> P
        pg = _get_mesh_axis_group(ctx.axis)
        world_size = _dist.dist.get_world_size(pg)

        mode = local_tensor_mode()
        if mode is not None and isinstance(grad_out, LocalTensor):
            result = mode.tensor_map(
                grad_out,
                lambda r, t: _varying_to_partial_fwd(
                    t, world_size, ctx.split_dim, r, stack=ctx.stack
                ),
            )
            return result, None, None, None
        else:
            rank = _dist.dist.get_rank(pg)
            return (
                _varying_to_partial_fwd(
                    grad_out, world_size, ctx.split_dim, rank, stack=ctx.stack
                ),
                None,
                None,
                None,
            )


class _ConvertInvariantToVarying(torch.autograd.Function):
    """convert(I,V): I -> V, backward is all_gather(I): V -> I."""

    @staticmethod
    def forward(ctx, x, axis, split_dim, stack):
        ctx.axis = axis
        ctx.split_dim = split_dim
        ctx.stack = stack
        pg = _get_mesh_axis_group(axis)
        world_size = _dist.dist.get_world_size(pg)

        mode = local_tensor_mode()
        if mode is not None and isinstance(x, LocalTensor):
            return mode.tensor_map(
                x,
                lambda r, t: _replicate_to_varying_fwd(
                    t, world_size, split_dim, r, stack=stack
                ),
            )
        else:
            rank = _dist.dist.get_rank(pg)
            return _replicate_to_varying_fwd(
                x, world_size, split_dim, rank, stack=stack
            )

    @staticmethod
    def backward(ctx, grad_out):
        # backward is all_gather(I): V -> I
        from sixlib.spmd_types._collectives import all_gather  # @manual

        src = V if ctx.stack else Shard(ctx.split_dim)
        return all_gather(grad_out, ctx.axis, src=src, dst=I), None, None, None


class _ConvertReplicateToPartial(torch.autograd.Function):
    """convert(R,P): R -> P, backward is convert(R,P): R -> P."""

    @staticmethod
    def forward(ctx, x, axis):
        ctx.axis = axis
        pg = _get_mesh_axis_group(axis)

        mode = local_tensor_mode()
        if mode is not None and isinstance(x, LocalTensor):
            return mode.tensor_map(x, lambda r, t: _replicate_to_partial_fwd(t, r))
        else:
            rank = _dist.dist.get_rank(pg)
            return _replicate_to_partial_fwd(x, rank)

    @staticmethod
    def backward(ctx, grad_out):
        # backward is same operation: convert(R,P)
        pg = _get_mesh_axis_group(ctx.axis)

        mode = local_tensor_mode()
        if mode is not None and isinstance(grad_out, LocalTensor):
            return mode.tensor_map(
                grad_out, lambda r, t: _replicate_to_partial_fwd(t, r)
            ), None
        else:
            rank = _dist.dist.get_rank(pg)
            return _replicate_to_partial_fwd(grad_out, rank), None


class _ConvertInvariantToPartial(torch.autograd.Function):
    """convert(I,P): I -> P, backward is reinterpret(R,I): R -> I (no-op)."""

    @staticmethod
    def forward(ctx, x, axis):
        ctx.axis = axis
        pg = _get_mesh_axis_group(axis)

        mode = local_tensor_mode()
        if mode is not None and isinstance(x, LocalTensor):
            return mode.tensor_map(x, lambda r, t: _replicate_to_partial_fwd(t, r))
        else:
            rank = _dist.dist.get_rank(pg)
            return _replicate_to_partial_fwd(x, rank)

    @staticmethod
    def backward(ctx, grad_out):
        # backward is reinterpret(R,I): R -> I (no-op)
        return grad_out, None


class _ConvertVaryingToPartial(torch.autograd.Function):
    """convert(V,P): V -> P, backward is convert(R,V): R -> V."""

    @staticmethod
    def forward(ctx, x, axis, split_dim, stack):
        ctx.axis = axis
        ctx.split_dim = split_dim
        ctx.stack = stack
        pg = _get_mesh_axis_group(axis)
        world_size = _dist.dist.get_world_size(pg)

        mode = local_tensor_mode()
        if mode is not None and isinstance(x, LocalTensor):
            return mode.tensor_map(
                x,
                lambda r, t: _varying_to_partial_fwd(
                    t, world_size, split_dim, r, stack=stack
                ),
            )
        else:
            rank = _dist.dist.get_rank(pg)
            return _varying_to_partial_fwd(x, world_size, split_dim, rank, stack=stack)

    @staticmethod
    def backward(ctx, grad_out):
        # backward is convert(R,V): R -> V (take local slice)
        pg = _get_mesh_axis_group(ctx.axis)
        world_size = _dist.dist.get_world_size(pg)

        mode = local_tensor_mode()
        if mode is not None and isinstance(grad_out, LocalTensor):
            result = mode.tensor_map(
                grad_out,
                lambda r, t: _replicate_to_varying_fwd(
                    t, world_size, ctx.split_dim, r, stack=ctx.stack
                ),
            )
            return result, None, None, None
        else:
            rank = _dist.dist.get_rank(pg)
            return (
                _replicate_to_varying_fwd(
                    grad_out, world_size, ctx.split_dim, rank, stack=ctx.stack
                ),
                None,
                None,
                None,
            )


def convert(  # noqa: C901
    x,
    axis: str | ProcessGroup,
    *,
    src: PerMeshAxisSpmdType,
    dst: PerMeshAxisSpmdType,
    expert_mode: bool = False,
):
    """``convert(x, mesh_axis, src, dst)``

    Convert from one local SPMD to another while preserving the semantics of
    the tensor, without doing communications.  When src/dst is shard, this
    means preserving the global SPMD semantics of the tensor per this sharding;
    when src/dst is varying, this means preserving the local SPMD semantics (we
    simply say that a non-varying tensor can be interpreted as varying by
    unbinding dim 0, following the natural behavior of collectives like
    all-gather and reduce-scatter that stack/unbind on dim 0).  The
    shard/varying conversions are actually exactly identical, except for being
    rank-preserving or not, so in the summary tables we will only include the
    varying versions of operations.  However, we include both in the API
    description for clarity.

    You cannot convert out of P: the only way to eliminate the pending
    reduction is to do the actual all-reduce.

    For convenience, we also support ``convert(R,I)`` and ``convert(I,R)``,
    which have the same meaning as ``reinterpret(R,I)`` and
    ``reinterpret(I,R)``.

    Args:
        x: Input tensor
        axis: The mesh axis to operate on (string name or ProcessGroup)
        src: Source local SPMD type (R, I, V, P, or S(i))
        dst: Destination local SPMD type (R, I, V, P, or S(i)).
             When src or dst is S(i), the split/concat dimension is
             derived from the shard index.  Plain V always uses dim 0.

    **Supported conversions:**

    ``convert(R,V): R -> V``, the backward is ``convert(V,P): V -> P``

    Input is replicated across ranks, so each rank holds the full tensor.  The
    output keeps only the local shard along tensor dim 0, producing a varying
    value.  This operation reduces the rank of the tensor.

    ::

        def convert_R_V_spec(x: f32[mesh_axis_size, *shape]) -> List[f32[*shape]]:
            return x.unbind()

        Forward:
                       A
        [A, B, C]  =>  B
                       C

        Backward:
        +[A, 0, 0]      A
        +[0, B, 0]  <=  B
        +[0, 0, C]      C

    ``convert(R,S(i)): R -> S(i)``, the backward is ``convert(S(i),P): S(i) -> P``

    Like above, but the rank of the tensor is not reduced, and an arbitrary
    tensor dim can be specified to be sharded.

    ::

        def convert_R_S_spec(x, i):
            return x.chunk(mesh_axis_size, i)

        Forward (for i = 0):
                                      [A0, A1]
        [A0, A1, B0, B1, C0, C1]  =>  [B0, B1]
                                      [C0, C1]

        Backward (for i = 0):
        +[A0, A1, 0,  0,  0,  0 ]      [A0, A1]
        +[0,  0,  B0, B1, 0,  0 ]  <=  [B0, B1]
        +[0,  0,  0,  0,  C0, C1]      [C0, C1]

    ``convert(I,V): I -> V``, the backwards is ``all_gather(V,I): V -> I``

    Input is invariant across ranks, so each rank holds the full tensor.  The
    output keeps only the local slice along dim 0, producing a varying value.
    The rank of the tensor is reduced.

    ::

        def convert_I_V_spec(x: f32[mesh_axis_size, *shape]) -> List[f32[*shape]]:
            return x.unbind()

        Forward:
                       A
        [A, B, C]  =>  B
                       C

        Backward:
                       A
        [A, B, C]  <=  B
                       C

    A common use case: shard a parameter (I) across ranks for memory savings.
    Each rank stores and computes on only its local slice; in backward, the
    per-rank gradient shards are all-gathered to reconstruct the full invariant
    gradient needed by the optimizer.

    ``convert(I,S(i)): I -> S(i)``, the backwards is ``all_gather(S(i),I): S(i) -> I``

    Like above, but the rank of the tensor is not reduced, and an arbitrary
    tensor dim can be specified to be sharded.

    ::

        def convert_I_S_spec(x, i):
            return x.chunk(mesh_axis_size, i)

        Forward (for i = 0):
                                      [A0, A1]
        [A0, A1, B0, B1, C0, C1]  =>  [B0, B1]
                                      [C0, C1]

        Backward (for i = 0):
                                      [A0, A1]
        [A0, A1, B0, B1, C0, C1]  <=  [B0, B1]
                                      [C0, C1]

    ``convert(R,P): R -> P``, the backwards is ``convert(R,P): R -> P``

    Input is replicated across ranks.  The output keeps the same per-rank tensor
    shape, but all ranks except the first are zeroed out, producing a partial
    value that sums to the original tensor after a cross-rank reduction.

    ::

        def convert_R_P_spec(x: f32[*shape]) -> f32[*shape]:
            return x

        Forward:
                 +[A]
        [A]  =>  +[0]
                 +[0]

        Backward:
        +[A]
        +[0]  <=  [A]
        +[0]

    This operation is its own backward (self-dual).  A common forward use
    case: when a replicated scalar (e.g., a regularization term) needs to be
    added to a partial loss without being counted N times.  Converting R -> P
    puts the value on only one rank, so the subsequent all-reduce produces
    the correct total.

    ``convert(I,P): I -> P``, the backwards is ``reinterpret(R,I): R -> I``

    Input is invariant across ranks.  The output keeps the same per-rank tensor
    shape, but all ranks except the first are zeroed out, producing a partial
    value that sums to the original tensor after a cross-rank reduction.

    ::

        def convert_I_P_spec(x: f32[*shape]) -> f32[*shape]:
            return x

        Forward:
                 +[A]
        [A]  =>  +[0]
                 +[0]

        Backward:
        [A]  <=  [A]

    ``convert(V,P): V -> P``, the backwards is ``convert(R,V): R -> V``

    Input is varying, with each rank holding a shard or distinct value.  The
    output places each rank's value into a disjoint position of a partial
    tensor (zeros elsewhere) so that summing across ranks reconstructs the
    stacked value.  The rank of the tensor is increased.

    ::

        def convert_V_P_spec(xs: List[f32[*shape]]) -> f32[mesh_axis_size, *shape]:
            return torch.stack(xs)

        Forward:
        A      +[A, 0, 0]
        B  =>  +[0, B, 0]
        C      +[0, 0, C]

        Backward:
        A
        B  <=  [A, B, C]
        C

    ``convert(S(i),P): S(i) -> P``, the backwards is ``convert(R,S(i)): R -> S(i)``

    Like above, but the rank of the tensor is not reduced, and an arbitrary
    tensor dim can be specified to be scattered on.

    ::

        def convert_S_P_spec(xs, i):
            return torch.concat(xs, i)

        Forward (for i = 0):
        [A0, A1]      +[A0, A1, 0,  0,  0,  0 ]
        [B0, B1]  =>  +[0,  0,  B0, B1, 0,  0 ]
        [C0, C1]      +[0,  0,  0,  0,  C0, C1]

        Backward (for i = 0):
        [A0, A1]
        [B0, B1]  <=  [A0, A1, B0, B1, C0, C1]
        [C0, C1]

    Here is a table of permissible converts (``-`` is no-op, ``O`` is
    supported, ``X`` is when the semantics is the same as reinterpret.)::

               dst
               R I V P
        src R  - X O O
            I  X - O O
            V      - O
            P        -
    """
    if has_torch_function_unary(x):
        return handle_torch_function(
            convert,
            (x,),
            x,
            axis,
            src=src,
            dst=dst,
            expert_mode=expert_mode,
        )
    # Canonicalize negative Shard dims
    src = _canonicalize_shard(src, x.ndim)
    dst = _canonicalize_shard(dst, x.ndim)

    # Derive dim from Shard types (V always uses dim 0)
    dim = 0
    if isinstance(src, Shard):
        dim = src.dim
    if isinstance(dst, Shard):
        dim = dst.dim

    # Normalize Shard to V for dispatch (they use the same underlying functions)
    src_base = V if isinstance(src, Shard) else src
    dst_base = V if isinstance(dst, Shard) else dst

    # Gate expert-only coercions
    if not expert_mode:
        if src_base is I and dst_base is P:
            raise ValueError(
                "convert(I, P) requires expert_mode=True. "
                "This zeros out all non-rank-0 tensors, which is rarely what you want. "
                "It exists as a backward for reinterpret(R, I)."
            )
        if src_base is V and dst_base is P:
            raise ValueError(
                f"convert({src}, P) requires expert_mode=True. "
                f"This pads with zeros to create a partial tensor, which is rarely what you "
                f"want in the forward pass. It exists as a backward for convert(R, {src})."
            )

    if src_base is dst_base:
        if isinstance(src, Shard) and isinstance(dst, Shard) and src.dim != dst.dim:
            raise ValueError(
                f"convert(S({src.dim}), S({dst.dim})) cannot change the shard dimension "
                f"without communication. Use all_to_all() instead."
            )
        return x  # no-op

    if src_base is R and dst_base is V:
        stack = not isinstance(dst, Shard)
        return _ConvertReplicateToVarying.apply(x, axis, dim, stack)
    elif src_base is R and dst_base is P:
        return _ConvertReplicateToPartial.apply(x, axis)
    elif src_base is R and dst_base is I:
        # Same as reinterpret
        return _ReplicateToInvariant.apply(x, axis)
    elif src_base is I and dst_base is V:
        stack = not isinstance(dst, Shard)
        return _ConvertInvariantToVarying.apply(x, axis, dim, stack)
    elif src_base is I and dst_base is P:
        return _ConvertInvariantToPartial.apply(x, axis)
    elif src_base is I and dst_base is R:
        # Same as reinterpret
        return _InvariantToReplicate.apply(x, axis)
    elif src_base is V and dst_base is P:
        stack = not isinstance(src, Shard)
        return _ConvertVaryingToPartial.apply(x, axis, dim, stack)
    else:
        if src_base is P:
            if dst_base is R or dst_base is I:
                raise ValueError(
                    f"convert({src}, {dst}) is not supported. "
                    f"Use all_reduce(src=P, dst={dst}) to perform the reduction and get the full sum."
                )
            elif dst_base is V or isinstance(dst, Shard):
                raise ValueError(
                    f"convert({src}, {dst}) is not supported. "
                    f"Use reduce_scatter(src=P, dst={dst}) to perform the reduction and get shards of the sum."
                )
            else:
                raise ValueError(
                    f"convert({src}, {dst}) is not supported. Cannot convert out of P."
                )
        else:
            raise ValueError(f"convert({src}, {dst}) is not supported.")


def shard(
    x,
    axis: str | ProcessGroup,
    *,
    src: PerMeshAxisSpmdType,
    dst: PerMeshAxisSpmdType,
):
    """Convenience alias: ``shard(src, S(i))`` is ``convert(src, S(i))``.

    Shards a replicated or invariant tensor along a given dimension without
    communication.

    Args:
        x: Input tensor
        axis: The mesh axis to operate on (string name or ProcessGroup)
        src: Source local SPMD type (R or I)
        dst: Destination shard type, must be S(i)
    """
    if not isinstance(dst, Shard):
        raise ValueError(f"shard dst must be S(i), got {dst}")
    return convert(x, axis, src=src, dst=dst)


def invariant_to_replicate(
    x,
    axis: str | ProcessGroup,
):
    """Convenience alias: ``invariant_to_replicate`` is ``reinterpret(I, R)``.

    Reinterprets an invariant tensor as replicated.  The local tensor is
    unchanged; the only effect is that the backward will perform an
    all-reduce (P -> I) instead of expecting invariant gradients.

    Two common use cases:

    1. A replicated parameter (I@tp, e.g., norm weights) entering
       computation where each rank may contribute a different gradient.
       The backward all-reduce synchronizes the gradient for the optimizer.
    2. Megatron ``CopyToModelParallelRegion`` (SP=False only): inter-block
       activations are I@tp, and this op transitions them to R@tp at the
       column-parallel linear entry.  With sequence parallelism (the common
       case), this role is filled by ``all_gather(dst=R): V->R`` instead.

    Args:
        x: Input tensor with I type on the mesh axis
        axis: The mesh axis to operate on (string name or ProcessGroup)
    """
    return reinterpret(x, axis, src=I, dst=R)
